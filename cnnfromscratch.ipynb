{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:19.987546489Z",
     "start_time": "2023-10-10T05:02:16.584798756Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 08:02:17.628194: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:19.993822610Z",
     "start_time": "2023-10-10T05:02:19.988040014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base Layer\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input): # returns output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate): # returns input\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Dense Layer\n",
    "\n",
    "#### Forward\n",
    "\n",
    "$$\n",
    "y_i = \\sum_{k=1}^{i} x_k w_{jk} + b_j\n",
    "$$\n",
    "\n",
    "#### Backward\n",
    "\n",
    "$$\n",
    "W \\gets W - \\gamma\\frac{\\partial E}{\\partial Y}X^t \\\\\n",
    "B \\gets B - \\gamma\\frac{\\partial E}{\\partial Y} \\\\\n",
    "\\frac{\\partial E}{\\partial X}=W^t\\frac{\\partial E}{\\partial Y}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:19.994265281Z",
     "start_time": "2023-10-10T05:02:19.991273392Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias         # Y = WX + B\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):             # returns input\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)    # Gradient of the loss w.r.t. weights, computed using the chain rule\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)    # Computing and returning the gradient of the loss w.r.t. the input for further backpropagation\n",
    "        self.weights -= learning_rate * weights_gradient            # Updating the weights using the computed gradient and learning rate\n",
    "        self.bias -= learning_rate * output_gradient                # Updating the biases using the gradient of the loss w.r.t. biases and learning rate\n",
    "        return input_gradient                                       # output_gradient is the gradient of the loss w.r.t. the output of this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.000358382Z",
     "start_time": "2023-10-10T05:02:19.997129615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Convolutional Layer\n",
    "\n",
    "#### Forward Computations\n",
    "\n",
    "$$\n",
    "Y_i = B_i + \\sum_{j=1}^n X_j \\star K_{ij}\n",
    "$$\n",
    "\n",
    "#### Backward Computations\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial K_{ij}} = X_j \\star \\frac{\\partial E}{\\partial Y_i} \\\\\n",
    "\\frac{\\partial E}{\\partial X_j} = \\sum_{i=1}^n \\frac{\\partial E}{\\partial Y_i} \\underset{\\text{full}}{\\star} K_{ij} \\\\\n",
    "K \\gets K - \\gamma \\frac{\\partial E}{\\partial K} \\\\\n",
    "B \\gets B - \\gamma \\frac{\\partial E}{\\partial Y} = B - \\gamma \\frac{\\partial E}{\\partial B}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.006474184Z",
     "start_time": "2023-10-10T05:02:20.003697101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, depth: int):\n",
    "        # input_shape shows the depth, height, and width of the input\n",
    "        # kernel_size represents the size of each matrix inside each kernel\n",
    "        # depth represents how many kernels we want and also the depth of the output\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "            # depth is the number of kernels\n",
    "            # input_height - kernel_size + 1 is the height of the output matrix\n",
    "            # input_width - kernel_size + 1 is the width of the output matrix\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "            # depth is the number of kernels\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "            # the * operator unpacks the tuple\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)      # Bi\n",
    "        for i in range(self.depth):             # Yi\n",
    "            for j in range(self.input_depth):   # Xj * Kij\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")    # order matters\n",
    "        return self.output                      # Yi = Bi + SUM Xj * Kij\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")    # Xj * dE/dYi\n",
    "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")          # dE/dXj = SUM dE/dYi FULL * Kij\n",
    "\n",
    "        self.kernels -= learning_rate * kernels_gradient        # Update kernels and\n",
    "        self.biases -= learning_rate * output_gradient          # biases using gradient descent\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.052793379Z",
     "start_time": "2023-10-10T05:02:20.007565178Z"
    }
   },
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)         # Reshape input to output shape\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)   # Reshape output to input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.053177721Z",
     "start_time": "2023-10-10T05:02:20.052645286Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.054001101Z",
     "start_time": "2023-10-10T05:02:20.052965012Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        def sigmoid_prime(x):\n",
    "            return sigmoid(x) * (1 - sigmoid(x))\n",
    "        \n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.184415777Z",
     "start_time": "2023-10-10T05:02:20.053139580Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_binary_data(x, y, limit):\n",
    "    zero_idx = np.where(y == 0)[0][:limit]\n",
    "    one_idx = np.where(y == 1)[0][:limit]\n",
    "    all_idxs = np.hstack((zero_idx, one_idx))\n",
    "    all_idxs = np.random.permutation(all_idxs)\n",
    "    x, y = x[all_idxs], y[all_idxs]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(len(y), 2, 1)\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_binary_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_binary_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:20.194872431Z",
     "start_time": "2023-10-10T05:02:20.185535482Z"
    }
   },
   "outputs": [],
   "source": [
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 100),\n",
    "    Sigmoid(),\n",
    "    Dense(100, 2),\n",
    "    Sigmoid()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:47.018958617Z",
     "start_time": "2023-10-10T05:02:20.198322496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error = 0.3064465619505612\n",
      "2/100, error = 0.09508077648739185\n",
      "3/100, error = 0.04380327041703627\n",
      "4/100, error = 0.028564181635172613\n",
      "5/100, error = 0.019934598211117607\n",
      "6/100, error = 0.015327532035199529\n",
      "7/100, error = 0.010520618477525712\n",
      "8/100, error = 0.008596969042119016\n",
      "9/100, error = 0.007196816213966001\n",
      "10/100, error = 0.006195403452882778\n",
      "11/100, error = 0.00558478660318953\n",
      "12/100, error = 0.005162743069800224\n",
      "13/100, error = 0.0047911285428213395\n",
      "14/100, error = 0.004439936965924434\n",
      "15/100, error = 0.004106478323505728\n",
      "16/100, error = 0.0038103923523334247\n",
      "17/100, error = 0.003505785053200543\n",
      "18/100, error = 0.003223389366941668\n",
      "19/100, error = 0.0029649876421655743\n",
      "20/100, error = 0.002739950364276919\n",
      "21/100, error = 0.0025546033402801833\n",
      "22/100, error = 0.0023983835942183716\n",
      "23/100, error = 0.0022597088209570172\n",
      "24/100, error = 0.0021337211942454573\n",
      "25/100, error = 0.0020188476273399055\n",
      "26/100, error = 0.0019153144161252506\n",
      "27/100, error = 0.0018233786728813692\n",
      "28/100, error = 0.0017421435282892767\n",
      "29/100, error = 0.001669835761603692\n",
      "30/100, error = 0.001604620931348231\n",
      "31/100, error = 0.0015450084696001997\n",
      "32/100, error = 0.001489884304607928\n",
      "33/100, error = 0.0014384348161165079\n",
      "34/100, error = 0.0013900752408118012\n",
      "35/100, error = 0.00134439916941102\n",
      "36/100, error = 0.0013011397165015367\n",
      "37/100, error = 0.0012601341125286355\n",
      "38/100, error = 0.001221289168738476\n",
      "39/100, error = 0.001184547859838509\n",
      "40/100, error = 0.0011498600453660676\n",
      "41/100, error = 0.001117162479463238\n",
      "42/100, error = 0.001086370643426669\n",
      "43/100, error = 0.0010573800827806934\n",
      "44/100, error = 0.0010300726371836072\n",
      "45/100, error = 0.0010043236663594452\n",
      "46/100, error = 0.0009800081215067973\n",
      "47/100, error = 0.0009570047401140985\n",
      "48/100, error = 0.0009351984347163025\n",
      "49/100, error = 0.0009144812462171816\n",
      "50/100, error = 0.000894752257983714\n",
      "51/100, error = 0.0008759167774319569\n",
      "52/100, error = 0.0008578849737929181\n",
      "53/100, error = 0.0008405700506157437\n",
      "54/100, error = 0.0008238859419195612\n",
      "55/100, error = 0.0008077444622261366\n",
      "56/100, error = 0.0007920518437010588\n",
      "57/100, error = 0.0007767047443698313\n",
      "58/100, error = 0.0007615863164871814\n",
      "59/100, error = 0.0007465642187394939\n",
      "60/100, error = 0.0007314953020978975\n",
      "61/100, error = 0.0007162468464259732\n",
      "62/100, error = 0.0007007495688978237\n",
      "63/100, error = 0.0006850881453763566\n",
      "64/100, error = 0.0006695781011992766\n",
      "65/100, error = 0.0006547023591686385\n",
      "66/100, error = 0.0006408743728001236\n",
      "67/100, error = 0.0006282445787095192\n",
      "68/100, error = 0.0006167260925507227\n",
      "69/100, error = 0.0006061330745889516\n",
      "70/100, error = 0.00059628108660881\n",
      "71/100, error = 0.0005870228358318517\n",
      "72/100, error = 0.000578249827376502\n",
      "73/100, error = 0.0005698839564210154\n",
      "74/100, error = 0.0005618687179589365\n",
      "75/100, error = 0.0005541624604217007\n",
      "76/100, error = 0.0005467336934336211\n",
      "77/100, error = 0.0005395579334515333\n",
      "78/100, error = 0.0005326155985757914\n",
      "79/100, error = 0.0005258905965472895\n",
      "80/100, error = 0.0005193693677072926\n",
      "81/100, error = 0.0005130402281199784\n",
      "82/100, error = 0.0005068929128266493\n",
      "83/100, error = 0.0005009182542022308\n",
      "84/100, error = 0.0004951079526456829\n",
      "85/100, error = 0.0004894544110846688\n",
      "86/100, error = 0.00048395061401193144\n",
      "87/100, error = 0.00047859003785174304\n",
      "88/100, error = 0.0004733665835228767\n",
      "89/100, error = 0.00046827452482716027\n",
      "90/100, error = 0.00046330846819261375\n",
      "91/100, error = 0.00045846332061987125\n",
      "92/100, error = 0.00045373426360388586\n",
      "93/100, error = 0.0004491167314518655\n",
      "94/100, error = 0.0004446063928753536\n",
      "95/100, error = 0.00044019913505674265\n",
      "96/100, error = 0.0004358910496180797\n",
      "97/100, error = 0.00043167842008029494\n",
      "98/100, error = 0.00042755771051423893\n",
      "99/100, error = 0.00042352555516522906\n",
      "100/100, error = 0.0004195787488892128\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    error = 0\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        error += binary_cross_entropy(y, output)\n",
    "\n",
    "        grad = binary_cross_entropy_prime(y, output)\n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "    error /= len(x_train)\n",
    "    print(f\"{epoch + 1}/{epochs}, error = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:05:19.599710617Z",
     "start_time": "2023-10-10T05:05:19.502185713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t1, [[0.]\n",
      " [1.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n",
      "pred => \t0, [[1.]\n",
      " [0.]] <- True\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_test, y_test):\n",
    "    output = x\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    print(f\"pred => \\t{np.argmax(output)}, {np.argmax(y)} <- True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:47.330938138Z",
     "start_time": "2023-10-10T05:02:47.106086715Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def importImage(image_location: str):\n",
    "    with Image.open(image_location) as img:\n",
    "        image = img.convert('L')\n",
    "        width, height = image.size\n",
    "        values = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                values[y][x] = image.getpixel((x,y))/255\n",
    "\n",
    "    values = values.reshape(1, height, width)\n",
    "\n",
    "    print(values.shape)\n",
    "\n",
    "    return values.astype(\"float32\")\n",
    "\n",
    "def determineValue(image_array):\n",
    "    output = image_array\n",
    "    plt.imshow(output[0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    print(f\"This is a {np.argmax(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:02:47.374178100Z",
     "start_time": "2023-10-10T05:02:47.332427282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 0\n"
     ]
    }
   ],
   "source": [
    "x = importImage(os.path.expanduser('~/Downloads/Untitled.png'))\n",
    "determineValue(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:03:52.985196329Z",
     "start_time": "2023-10-10T05:03:52.941171081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08235294 0.5254902  0.43529412 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36078432 1.         0.58431375 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.58431375 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.77254903 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15686275 0.9882353  0.8156863  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14117648 0.9882353  0.58431375 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.60784316 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10588235 0.9882353  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.04313726 0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.9843137  0.98039216 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.75686276 0.99607843 0.36078432 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12941177 0.6901961  0.37254903 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[92][0])\n",
    "plt.axis(\"off\")\n",
    "plt.show\n",
    "print(x_test[92][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "micrograd",
   "language": "python",
   "display_name": "micrograd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
