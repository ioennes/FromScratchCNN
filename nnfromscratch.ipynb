{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06935877",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6deb1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0237149",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5afbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Layer\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input): # returns output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate): # returns input\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f2654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer inherits from Base Layer\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias         # Y = WX + B\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):             # returns input\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)    # Gradient of the loss w.r.t. weights, computed using the chain rule\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)    # Computing and returning the gradient of the loss w.r.t. the input for further backpropagation\n",
    "        self.weights -= learning_rate * weights_gradient            # Updating the weights using the computed gradient and learning rate\n",
    "        self.bias -= learning_rate * output_gradient                # Updating the biases using the gradient of the loss w.r.t. biases and learning rate\n",
    "        return input_gradient                                       # output_gradient is the gradient of the loss w.r.t. the output of this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a42f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Layer inherits from Base Layer\n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79aa0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh Activation Function\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_prime = lambda x: 1 - np.tanh(x) ** 2\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cd0386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7796ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/10000\t error=0.00012169175324081647\n",
      "1000/10000\t error=5.131869007928736e-05\n",
      "1500/10000\t error=3.179126701487351e-05\n",
      "2000/10000\t error=2.2809762034287286e-05\n",
      "2500/10000\t error=1.769257508688477e-05\n",
      "3000/10000\t error=1.4474708648482511e-05\n",
      "3500/10000\t error=1.2300988773262742e-05\n",
      "4000/10000\t error=1.0572963990655387e-05\n",
      "4500/10000\t error=9.259986135201076e-06\n",
      "5000/10000\t error=8.405699656072939e-06\n",
      "5500/10000\t error=7.475428466972509e-06\n",
      "6000/10000\t error=6.778242055427937e-06\n",
      "6500/10000\t error=5.674644982959735e-05\n",
      "7000/10000\t error=5.754851455596804e-06\n",
      "7500/10000\t error=5.3251049445709665e-06\n",
      "8000/10000\t error=6.533152779058971e-06\n",
      "8500/10000\t error=4.672538583964776e-06\n",
      "9000/10000\t error=4.3743378320576285e-06\n",
      "9500/10000\t error=4.270621176687159e-06\n",
      "10000/10000\t error=4.0953461633043206e-06\n"
     ]
    }
   ],
   "source": [
    "# XOR\n",
    "\n",
    "X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))  # Input data reshaped to (4, 2, 1)\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))            # Output data reshaped to (4, 1, 1)\n",
    "\n",
    "# Neural Network Architecture\n",
    "network = [\n",
    "    Dense(2, 3),  # Dense layer with 2 inputs and 3 neurons\n",
    "    Tanh(),       # Hyperbolic tangent activation function\n",
    "    Dense(3, 1),  # Dense layer with 3 inputs (from previous layer) and 1 neuron\n",
    "    Tanh()        # Hyperbolic tangent activation function\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000      # Number of training epochs\n",
    "learning_rate = 0.1 # Learning rate for gradient descent\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    error = 0  # Initialize error for this epoch to 0\n",
    "    \n",
    "    for x, y in zip(X, Y):  # Iterate through each training example\n",
    "        \n",
    "        # Forward Pass\n",
    "        output = x  # Initialize output with input data\n",
    "        for layer in network:  # Pass output through each layer in the network\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        # Error Calculation\n",
    "        error += mse(y, output)  # Accumulate Mean Squared Error\n",
    "        \n",
    "        # Backward Pass (Backpropagation)\n",
    "        grad = mse_prime(y, output)  # Compute gradient of error w.r.t output\n",
    "        for layer in reversed(network):  # Pass gradient through each layer in reverse order\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "    \n",
    "    error /= len(X)  # Compute average error for this epoch\n",
    "    \n",
    "    error /= len(X)\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"{epoch+1}/{epochs}\\t error={error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57fe1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction \t => \t 1\n"
     ]
    }
   ],
   "source": [
    "def predict(network, input_data):\n",
    "    output = input_data\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "input_data = [[0], [1]]\n",
    "output_data = predict(network, input_data)\n",
    "\n",
    "print(f\"{input_data[0][0]} XOR {input_data[0][1]} \\t => \\t {round(output_data[0][0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
