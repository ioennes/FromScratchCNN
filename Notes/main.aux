\relax 
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Basic Concepts in Machine Learning}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Neurons}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Basic Structure}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Implementing a Threshold Logic Unit}{1}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Threshold Logic Unit}}{1}{}\protected@file@percent }
\newlabel{alg:TLU}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Activation Functions}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Different Properties of Activation Functions}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nonlinearity.}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Range.}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Continuously Differentiable.}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Mathematical Details and Activation Functions}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Gradient Descent}{3}{}\protected@file@percent }
\newlabel{eq:monotonicgradient}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Learning Rate}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Loss Function}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Quadratic Loss Function}{4}{}\protected@file@percent }
\newlabel{eq:SEL}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}0-1 Loss Function}{4}{}\protected@file@percent }
\newlabel{eq:01loss}{{3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Building a Neural Network}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Implementation Design}{5}{}\protected@file@percent }
\newlabel{eq:derivativeoferrorwrtparameter}{{4}{5}}
\newlabel{eq:derivativeoferrorwrtinput}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Dense Layer}{5}{}\protected@file@percent }
\newlabel{eq:outputsum}{{6}{5}}
\newlabel{eq:outputsummatrix}{{7}{6}}
\newlabel{eq:errors}{{8}{6}}
\newlabel{eq:errorwrtweightsandbiases}{{9}{6}}
\newlabel{eq:errorwrtbias}{{10}{6}}
\newlabel{eq:errorwrtoutput}{{11}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The Activation Layer}{7}{}\protected@file@percent }
\newlabel{eq:forwardactivation}{{12}{7}}
\newlabel{eq:errorwrtoutputactivation}{{13}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Building a Convolutional Neural Network}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Cross-Corrolation and Convolution}{8}{}\protected@file@percent }
\newlabel{eq:validcrosscorrolation}{{14}{8}}
\newlabel{eq:convolution}{{15}{8}}
\newlabel{eq:fullcrosscorrolation}{{16}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Convolutional Layer}{8}{}\protected@file@percent }
\newlabel{eq:forwardpropagationoftheconvolutionallayer}{{17}{8}}
\newlabel{eq:kernelbackpropagation}{{20}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Cross-Entropy}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Binary Cross-Entropy Loss}{9}{}\protected@file@percent }
\newlabel{eq:binarycrossentropyloss}{{21}{9}}
\newlabel{eq:binarycrossentropylossprime}{{22}{9}}
\gdef \@abspage@last{11}
